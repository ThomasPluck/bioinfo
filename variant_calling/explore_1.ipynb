{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding VariantNET\n",
    "This is a Python 2 notebook where we will be taking code samples from [Jason Chin's VariantNet repository](https://github.com/pb-jchin/VariantNET). Finding out how data is loaded and the VariantNET itself is implemented - while proposing an alternative neural net.\n",
    "\n",
    "A digest of points to be covered:\n",
    "<ul>\n",
    "    <li>Loading and Parsing Data</li>\n",
    "    <li>Designing Models to Train Data</li>\n",
    "    <li>Evaluating Model Effectivity</li>\n",
    "    <li>Proposals for future development</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# for plotting to visualize some data\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at the formats of the data that are imported to train VariantNET and how they are parsed by VariantNet's utils. In the demo notebook - we can see that a single command `get_training_data` is used to return two numpy.ndarrays which stores training data and training labels how we would expect if we were to use this data to train a scikit-learn model.\n",
    "\n",
    "In more detail, the command that imports the data is:\n",
    "\n",
    "```python\n",
    "Xarray, Yarray, pos_array = \\\n",
    "utils.get_training_array(\"../wd/aln_tensor_chr21\", \n",
    "                         \"../wd/variants_chr21\", \n",
    "                         \"../testing_data/chr21/CHROM21_v.3.3.2_highconf_noinconsistent.bed\" )\n",
    "```\n",
    "\n",
    "Let's have a look at the files used in calling the function and see how they fit together to return the numpy ndarrays, that we can use for convenient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14060179 GCCCCCAACAGTACC 0.0 0.0 9.0 0.0 1.0 0.0 8.0 0.0 1.0 0.0 8.0 0.0 0.0 9.0 0.0 0.0 0.0 7.0 1.0 1.0 0.0 7.0 1.0 1.0 0.0 9.0 0.0 0.0 0.0 8.0 1.0 0.0 0.0 8.0 1.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 9.0 0.0 1.0 0.0 9.0 0.0 1.0 0.0 10.0 0.0 0.0 1.0 9.0 0.0 0.0 1.0 9.0 0.0 0.0 10.0 0.0 0.0 0.0 9.0 1.0 0.0 0.0 9.0 1.0 0.0 0.0 10.0 0.0 0.0 0.0 8.0 2.0 0.0 0.0 8.0 2.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 9.0 0.0 1.0 0.0 9.0 0.0 1.0 10.0 0.0 0.0 0.0 9.0 0.0 1.0 0.0 9.0 0.0 1.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 0.0 9.0 0.0 0.0 0.0 9.0 0.0 0.0 0.0 9.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0 0.0 8.0 0.0 0.0 0.0 8.0 0.0 0.0 0.0 8.0 0.0 0.0 0.0 10.0 0.0 0.0 1.0 9.0 0.0 0.0 1.0 9.0 0.0 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "with open('../wd/aln_tensor_chr21','r') as f:\n",
    "    line = f.readline()\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this data format fully, we must refer to Chin's [Medium article](https://towardsdatascience.com/simple-convolution-neural-network-for-genomic-variant-calling-with-tensorflow-c085dbc2026f). In it is stated:\n",
    "\n",
    "> For each candidate, we add 7 base flanking bases on both side.\n",
    "\n",
    "After checking in `utils.py`, we can confirm that the first number is the position of the candidate base. And the length of the sequence being 15 bases agrees with a single base being padded with 7 bases either side $(7+7+1=15)$.\n",
    "\n",
    "Finally we are left with what appears to be 180 arbitrary floats. Again checking with how `utils.py` functions, this encodes information which holds both the reference sequence in a one-hot encoded matrix and two other one-hot encoded matrices which show disparity between reference sequence and sequencer data.\n",
    "\n",
    "This gives us a way to display the above file in a format that we see in Chin's article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f5894127ed0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAD8CAYAAADNGFurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP+pocPundaV8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5894127f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line_list = line.strip().split()\n",
    "ref_seq = line_list[1]\n",
    "\n",
    "reshaped = np.reshape(np.array([float(i) for i in line_list[2:]]),\n",
    "                      (15,3,4))\n",
    "\n",
    "fig, axarr = plt.subplots(3,sharey=True,sharex=True)\n",
    "\n",
    "for i in range(3):\n",
    "    im = axarr[i].imshow(reshaped[:,i,:].T,cmap='hot')\n",
    "    \n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the Medium article:\n",
    "\n",
    "> We summary the alignment information into three 15 x 4 matrices... One matrix track the baseline encoding the reference sequence and the number of supported reads through a position. Two other 15 by 4 matrix track the differences between the sequencing data and the references.\n",
    "\n",
    "The differences between sequencing data and references isn't apparent here, since the reference hasn't been subtracted from the sequencing data. However, this is how it present in the train data below.\n",
    "\n",
    "Next, let us analyze `\"../wd/variants_chr21\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10270925 T A 0 1\n",
      "\n",
      "12996850 C G 0 1\n",
      "\n",
      "14069696 T A 0 1\n",
      "\n",
      "14069739 G A 0 1\n",
      "\n",
      "14071090 C A 0 1\n",
      "\n",
      "14071143 T G 0 1\n",
      "\n",
      "14071850 ATC T 1 1\n",
      "\n",
      "14071923 C CCCCCCT 0 1\n",
      "\n",
      "14071983 T G 1 1\n",
      "\n",
      "14072221 G A 0 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../wd/variants_chr21','r') as f:\n",
    "    lines = f.readlines()[:10]\n",
    "    for line in lines:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests we have some obvious variants here. When we read `utils.py`, we have this code snippet with regards to how this data is parsed:\n",
    "\n",
    "```python\n",
    "for row in f:\n",
    "            row = row.strip().split()\n",
    "            if row[3] == \"0\":\n",
    "                het = True\n",
    "            else:\n",
    "                het = False\n",
    "```\n",
    "\n",
    "This indicates that the second number from the left determines whether or not the described variant is heterozygous. Delving deeper, the variants in this file are split in VariantNET into 4 categories - heterozygous, homozygous and non-SNP (this is for cases with multiple polymorphisms longer than a single base).\n",
    "\n",
    "We are now ready to explore the final file used to generate training data: `../testing_data/chr21/CHROM21_v.3.3.2_highconf_noinconsistent.bed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr21\t10269870\t10269950\n",
      "\n",
      "chr21\t10270051\t10270144\n",
      "\n",
      "chr21\t10270409\t10270716\n",
      "\n",
      "chr21\t10270914\t10271075\n",
      "\n",
      "chr21\t10271176\t10271201\n",
      "\n",
      "chr21\t10271390\t10271604\n",
      "\n",
      "chr21\t10272048\t10272058\n",
      "\n",
      "chr21\t10272350\t10272468\n",
      "\n",
      "chr21\t10272569\t10273022\n",
      "\n",
      "chr21\t10273193\t10273450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../testing_data/chr21/CHROM21_v.3.3.2_highconf_noinconsistent.bed\",'r') as f:\n",
    "    lines = f.readlines()[:10]\n",
    "    for line in lines:\n",
    "        print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data encodes from left to right, chromosomal location of a known variant, beginning and end sequence locations of known variants.\n",
    "\n",
    "Using a combination of all these three we are able to get a full set of training data for VariantNet. As we will now see in the break down of `get_training_data` in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This provides a simple one-hot encoder for base-letters\n",
    "base2num = dict(zip(\"ACGT\",(0, 1, 2, 3)))\n",
    "\n",
    "import intervaltree\n",
    "#Create an interval search tree using the intervaltree module\n",
    "tree =  intervaltree.IntervalTree()\n",
    "\n",
    "#Open high confidence intervals for variants\n",
    "with open(\"../testing_data/chr21/CHROM21_v.3.3.2_highconf_noinconsistent.bed\") as f:\n",
    "    #Iterate over rows\n",
    "    for row in f:\n",
    "        #Strip and split row by white-space\n",
    "        row = row.strip().split()\n",
    "        #Beginning of interval encoded by the first number\n",
    "        b = int(row[1])\n",
    "        #End of interval encoded by second number\n",
    "        e = int(row[2])\n",
    "        #Add start and end indices to our tree\n",
    "        tree.addi(b, e, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with an intervaltree using information stored in the bed about intervals containing variants. This is then populated us preparing us for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dict to store train labels.\n",
    "Y_intitial = {}\n",
    "\n",
    "#Open known variants\n",
    "with open(\"../wd/variants_chr21\") as f:\n",
    "    #Iterate over rows\n",
    "    for row in f:\n",
    "        #Strip and split by white space\n",
    "        row = row.strip().split()\n",
    "        #If second number is zero: then variant is heterozygous\n",
    "        if row[3] == \"0\":\n",
    "            het = True\n",
    "        else:\n",
    "            het = False\n",
    "\n",
    "        #Position is denoted by first number\n",
    "        pos = int(row[0])\n",
    "        \n",
    "        #If the position is not in our tree\n",
    "        if len(tree.search(pos)) == 0:\n",
    "            #Forget the variant\n",
    "            continue\n",
    "        \n",
    "        #Define a list as our label-vector\n",
    "        base_vec = [0,0,0,0,0,0,0,0]  #first 4, base vec, last 4, het, hom, non-variant, not-SNPs\n",
    "        \n",
    "        #If heterozygous\n",
    "        if het:\n",
    "            #Our variant is:\n",
    "            #Half reference base\n",
    "            base_vec[base2num[row[1][0]]] = 0.5\n",
    "            #Half variant base\n",
    "            base_vec[base2num[row[2][0]]] = 0.5\n",
    "            #All heterozygous\n",
    "            base_vec[4] = 1.\n",
    "        else:\n",
    "            #Otherwise it is homozygous:\n",
    "            #So it is all reference base\n",
    "            base_vec[base2num[row[2][0]]] = 1\n",
    "            #And marked for homozygosity\n",
    "            base_vec[5] = 1.\n",
    "\n",
    "        #If our variants are bigger than a single base\n",
    "        if len(row[1]) > 1 or len(row[2]) > 1 :  # not simple SNP case\n",
    "            #This is not a simple SNP case\n",
    "            base_vec[7] = 1.\n",
    "            #It is neither heterozygous nor homozygous\n",
    "            base_vec[4] = 0.\n",
    "            base_vec[5] = 0.\n",
    "\n",
    "        #Update label for this position as base_vec\n",
    "        Y_intitial[pos] = base_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here extract all positions and labels pertaining to variants of interest. A few conditions are used to distinguish heterozygous, homozygous and non-simple SNPs and return a dict with positions mapped to variant state vectors. We can also see that our tree from our previous step is used to invalidate candidate variants that are not within predefined intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort positions of labels\n",
    "Y_pos = sorted(Y_intitial.keys())\n",
    "\n",
    "#Initial position is cpos\n",
    "cpos = Y_pos[0]\n",
    "\n",
    "#Iterating over non-cpos positions\n",
    "for pos in Y_pos[1:]:\n",
    "    #If you are within 12 bases of cpos:\n",
    "    if abs(pos - cpos) < 12:\n",
    "        #Both pos and cpos variant candidate are not simple SNP\n",
    "        Y_intitial[pos][7] = 1\n",
    "        Y_intitial[cpos][7] = 1\n",
    "        #We are both not homozygous or heterozygous\n",
    "        Y_intitial[pos][4] = 0\n",
    "        Y_intitial[cpos][4] = 0\n",
    "        Y_intitial[pos][5] = 0\n",
    "        Y_intitial[cpos][5] = 0\n",
    "    #cpos is now previous position.\n",
    "    cpos = pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we now iterate over each one of our found variants - if they are 12 bases apart, we determine that they are either falsely homozygous or heterozygous and must in fact be non-simple SNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a dictionary to be filled with 15x3x4 tensors as\n",
    "#described above\n",
    "X_intitial = {}  \n",
    "\n",
    "#Open the first file we explored\n",
    "with open('../wd/aln_tensor_chr21','r') as f:\n",
    "    #Iterate over rows\n",
    "    for row in f:\n",
    "        #Strip string and split into list by white-space\n",
    "        row = row.strip().split()\n",
    "        #Position is the first split element of each row\n",
    "        pos = int(row[0])\n",
    "        #Reference sequence is the second\n",
    "        ref_seq = row[1]\n",
    "        #Use uppercase!\n",
    "        ref_seq = ref_seq.upper()\n",
    "        \n",
    "        #If our candidate variant is not an actual base:\n",
    "        if ref_seq[7] not in [\"A\",\"C\",\"G\",\"T\"]:\n",
    "            #Forget it.\n",
    "            continue\n",
    "\n",
    "        #Reshape our floats into an 15x3x4 tensor\n",
    "        vec = np.reshape(np.array([float(x) for x in row[2:]]),\n",
    "                         (15,3,4))\n",
    "        \n",
    "        #Transpose it so we have, sequence position, one-hot base,\n",
    "        #Reference vs. Sequencer Read\n",
    "        vec = np.transpose(vec, axes=(0,2,1))\n",
    "        \n",
    "        #If no base recorded at this location\n",
    "        if sum(vec[7,:,0]) < 5:\n",
    "            continue\n",
    "\n",
    "        #Sequencer reads are redefined as difference between\n",
    "        #themselves and reference sequence.\n",
    "        vec[:,:,1] -= vec[:,:,0]\n",
    "        vec[:,:,2] -= vec[:,:,0]\n",
    "\n",
    "        #Update X_intitial at position to tensor.\n",
    "        X_intitial[pos] = vec\n",
    "        \n",
    "        #If we can't find position of found sequence above\n",
    "        if pos not in Y_intitial:\n",
    "                #Candidate base is one\n",
    "                base_vec = [0,0,0,0,0,0,0,0]\n",
    "                base_vec[base2num[ref_seq[7]]] = 1\n",
    "                #This is a non-variant instance!\n",
    "                base_vec[6] = 1.\n",
    "                Y_intitial[pos] = base_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stage loads all our train data. A few caviats exist to prevent candidate variant bases which have either incorrectly labelled bases or no bases at all from passing into the training data and preventing an error.\n",
    "\n",
    "The final if statement is crucial, indicating that if a given instance of train data can't find a label in `Y_intitial` then it must be assigned a label to indicate that it is a non-variant base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Sort train data keys to list\n",
    "all_pos = sorted(X_intitial.keys())\n",
    "\n",
    "#Shuffle keys\n",
    "random.seed(1337)\n",
    "random.shuffle(all_pos)\n",
    "\n",
    "#Blank lists\n",
    "Xarray = []\n",
    "Yarray = []\n",
    "pos_array = []\n",
    "\n",
    "#Append to list in order defined by shuffle\n",
    "for pos in all_pos:\n",
    "    Xarray.append(X_intitial[pos])\n",
    "    Yarray.append(Y_intitial[pos])\n",
    "    pos_array.append(pos)\n",
    "\n",
    "#Load train data and train labels into numpy ndarrays\n",
    "Xarray = np.array(Xarray)\n",
    "Yarray = np.array(Yarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we shuffle the keys of the train data and then use this shuffling to dictate the order of a pair of numpy ndarrays. We now have format that we are more familiar with for implementing ML.\n",
    "\n",
    "We can now use this to train VariantNET or an alternative neural network. Here I will use a slight variation on VariantNET and we can test how effective this method is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 15, 4, 48)         1200      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 4, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 4, 48)         27696     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               344576    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 505,828\n",
      "Trainable params: 505,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(48, (2, 4), input_shape=(15, 4, 3), padding='same', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(48, (3, 4), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing\n",
      "Training model\n",
      "Epoch 1/25\n",
      "63152/63152 [==============================] - 47s 744us/step - loss: 0.6352 - acc: 0.7923\n",
      "Epoch 2/25\n",
      "63152/63152 [==============================] - 48s 753us/step - loss: 0.4122 - acc: 0.8696\n",
      "Epoch 3/25\n",
      "63152/63152 [==============================] - 48s 757us/step - loss: 0.3777 - acc: 0.8792\n",
      "Epoch 4/25\n",
      "63152/63152 [==============================] - 48s 757us/step - loss: 0.3584 - acc: 0.8841\n",
      "Epoch 5/25\n",
      "63152/63152 [==============================] - 48s 762us/step - loss: 0.3477 - acc: 0.8891\n",
      "Epoch 6/25\n",
      "63152/63152 [==============================] - 48s 760us/step - loss: 0.3357 - acc: 0.8928\n",
      "Epoch 7/25\n",
      "63152/63152 [==============================] - 50s 795us/step - loss: 0.3293 - acc: 0.8939\n",
      "Epoch 8/25\n",
      "63152/63152 [==============================] - 48s 761us/step - loss: 0.3218 - acc: 0.8981\n",
      "Epoch 9/25\n",
      "63152/63152 [==============================] - 43s 688us/step - loss: 0.3135 - acc: 0.9007\n",
      "Epoch 10/25\n",
      "63152/63152 [==============================] - 44s 689us/step - loss: 0.3084 - acc: 0.9012\n",
      "Epoch 11/25\n",
      "63152/63152 [==============================] - 44s 695us/step - loss: 0.3042 - acc: 0.9045\n",
      "Epoch 12/25\n",
      "63152/63152 [==============================] - 44s 691us/step - loss: 0.2984 - acc: 0.9050\n",
      "Epoch 13/25\n",
      "63152/63152 [==============================] - 44s 689us/step - loss: 0.2936 - acc: 0.9068\n",
      "Epoch 14/25\n",
      "63152/63152 [==============================] - 44s 690us/step - loss: 0.2894 - acc: 0.9086\n",
      "Epoch 15/25\n",
      "63152/63152 [==============================] - 44s 689us/step - loss: 0.2853 - acc: 0.9104\n",
      "Epoch 16/25\n",
      "63152/63152 [==============================] - 43s 688us/step - loss: 0.2800 - acc: 0.9115\n",
      "Epoch 17/25\n",
      "63152/63152 [==============================] - 43s 689us/step - loss: 0.2763 - acc: 0.9122\n",
      "Epoch 18/25\n",
      "63152/63152 [==============================] - 44s 691us/step - loss: 0.2735 - acc: 0.9135\n",
      "Epoch 19/25\n",
      "63152/63152 [==============================] - 44s 693us/step - loss: 0.2693 - acc: 0.9143\n",
      "Epoch 20/25\n",
      "63152/63152 [==============================] - 44s 690us/step - loss: 0.2662 - acc: 0.9151\n",
      "Epoch 21/25\n",
      "63152/63152 [==============================] - 44s 690us/step - loss: 0.2641 - acc: 0.9156\n",
      "Epoch 22/25\n",
      "63152/63152 [==============================] - 44s 690us/step - loss: 0.2599 - acc: 0.9171\n",
      "Epoch 23/25\n",
      "63152/63152 [==============================] - 44s 692us/step - loss: 0.2577 - acc: 0.9171\n",
      "Epoch 24/25\n",
      "63152/63152 [==============================] - 44s 692us/step - loss: 0.2534 - acc: 0.9186\n",
      "Epoch 25/25\n",
      "63152/63152 [==============================] - 44s 699us/step - loss: 0.2521 - acc: 0.9188\n",
      "Predicting on test data\n",
      "Generating results\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c8e16f5302d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generating results\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_results(y_true,y_pred):\n",
    "    return \n",
    "\n",
    "print('Splitting data into training and testing')\n",
    "Yarray2 = Yarray[:,4:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xarray, Yarray2, test_size=0.2, random_state=1337)\n",
    "\n",
    "print('Training model')\n",
    "model.fit(X_train, y_train,batch_size=64, epochs=25)\n",
    "\n",
    "print('Predicting on test data')\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Generating results\\n')\n",
    "print('Accuracy:',accuracy_score(y_test,y_pred))\n",
    "print('Precision:',precision_score(y_test,y_pred))\n",
    "print('Recall:',recall_score(y_test,y_pred))\n",
    "print('F1 Score:',f1_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
